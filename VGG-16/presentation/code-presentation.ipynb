{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e036b527-e921-485d-bbcd-a38315080257",
   "metadata": {},
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d50d8e3-79a7-4220-9628-1b549996e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def augment(n_images_per_class: int, \n",
    "            resolution: int, \n",
    "            seed: int,\n",
    "            input_path = \"raw_database\",\n",
    "            output_path = \"augmented-db\"):\n",
    "    \n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    \n",
    "    classes = os.listdir(input_path)\n",
    "    if not os.path.exists(output_path):\n",
    "            os.mkdir(output_path)\n",
    "    for Class in classes:\n",
    "        if not os.path.exists(os.path.join(output_path,Class)):\n",
    "            os.mkdir(os.path.join(output_path,Class))\n",
    "    \n",
    "    n_total_images = len(classes)*n_images_per_class\n",
    "    X = torch.zeros(n_total_images,3,resolution,resolution)\n",
    "    Y = torch.zeros(n_total_images)\n",
    "    transf = transforms.ToTensor()\n",
    "    \n",
    "    with open(os.path.join(output_path,\"data.txt\"),\"w\") as file:\n",
    "        i = 0\n",
    "        for label, Class in enumerate(classes):\n",
    "            images = os.listdir(os.path.join(input_path,Class))\n",
    "            n = len(images)\n",
    "            m = n_images_per_class // n\n",
    "            n_transf_per_image = [m for i in range(n - n_images_per_class % n)] + [m+1 for i in range(n_images_per_class % n)]\n",
    "            rng.shuffle(n_transf_per_image)\n",
    "            \n",
    "            for image_name,n_transf in zip(images,n_transf_per_image):\n",
    "                img = Image.open(os.path.join(input_path,Class,image_name)).convert(\"RGB\")\n",
    "                for j in range(n_transf):\n",
    "                    new_img, data = produce_image(img, resolution, rng)\n",
    "                    new_image_name = f\"{image_name[:-5]}_{j}.jpeg\" \n",
    "                    new_img.save(os.path.join(output_path,Class,new_image_name))\n",
    "                    X[i] = transf(new_img)\n",
    "                    Y[i] = label\n",
    "                    i += 1 \n",
    "                    \n",
    "                    file.write(new_image_name+\"\\n\")\n",
    "                    for dat in data:\n",
    "                        file.write(dat+\"\\n\")\n",
    "                    file.write(\"\\n\")\n",
    "                    \n",
    "                    if i % 100 == 0:\n",
    "                        print(f\"class {label+1}/{len(classes)}, total {i}/{n_total_images}\")\n",
    "        \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y,train_size=0.8)\n",
    "    torch.save(X_train,os.path.join(output_path,\"X_train.pt\"))\n",
    "    torch.save(X_test,os.path.join(output_path,\"X_test.pt\"))\n",
    "    torch.save(Y_train,os.path.join(output_path,\"Y_train.pt\"))\n",
    "    torch.save(Y_test,os.path.join(output_path,\"Y_test.pt\"))\n",
    "            \n",
    "    \n",
    "def produce_image(img, resolution, rng):\n",
    "    transformation_list = [(rotate,True),\n",
    "                           (crop,True),\n",
    "                           (hor_flip,False),\n",
    "                           (ver_flip,False),\n",
    "                           (blur,False),\n",
    "                           (brightness,False),\n",
    "                           (contrast,False),\n",
    "                           (color,False)]\n",
    "    \n",
    "    n_transformations = rng.choice([1,2,3,4],p=[0.2,0.4,0.3,0.1])\n",
    "    chosen_transformations = rng.choice(list(range(len(transformation_list))),\n",
    "                                              size=n_transformations,\n",
    "                                              replace=False)\n",
    "    \n",
    "    data = []\n",
    "    for i in chosen_transformations:\n",
    "        if transformation_list[i][1]:\n",
    "            img, dat = transformation_list[i][0](img, rng)\n",
    "            data += [dat]\n",
    "    \n",
    "    img = transforms.functional.center_crop(img,min(img.size))    \n",
    "    img = img.resize((resolution,resolution))\n",
    "    \n",
    "    for i in chosen_transformations:\n",
    "        if not transformation_list[i][1]:\n",
    "            img, dat = transformation_list[i][0](img, rng)\n",
    "            data += [dat]\n",
    "        \n",
    "    return [img, data]\n",
    "\n",
    "\n",
    "def hor_flip(img, rng):\n",
    "    img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    return [img, \"Horizontal Flip\"]\n",
    "\n",
    "def ver_flip(img, rng):\n",
    "    img = img.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "    return [img, \"Vertical Flip\"]\n",
    "\n",
    "def crop(img, rng):\n",
    "    width, height = img.size\n",
    "    scale = rng.uniform(0.5,0.75)\n",
    "    maxOffset = (1-scale)*min(width,height)\n",
    "    offsetX = rng.uniform(-1,1)\n",
    "    offsetY = rng.uniform(-1,1)\n",
    "    size = min(width,height)*scale\n",
    "    img = img.crop((width/2 + offsetX*maxOffset/2 - size/2, height/2 + offsetY*maxOffset/2 - size/2, width/2 + offsetX*maxOffset/2 + size/2, height/2 + offsetY*maxOffset/2 + size/2))\n",
    "    return [img, f\"Crop: scale = {scale}, offsetX = {offsetX}, offsetY = {offsetY}\"]\n",
    "\n",
    "def rotate(img, rng):\n",
    "    angle = rng.uniform(-45,45)\n",
    "    img = img.rotate(angle,expand=False)\n",
    "    return [img, f\"Rotate: angle = {angle}\"]\n",
    "\n",
    "def blur(img, rng):\n",
    "    value = rng.integers(1,3)\n",
    "    img = img.filter(ImageFilter.GaussianBlur(radius = value))\n",
    "    return [img, f\"Gaussian Blur: value = {value}\"]\n",
    "\n",
    "def brightness(img, rng):\n",
    "    value = rng.uniform(0.3,1.7)\n",
    "    enhancer = ImageEnhance.Brightness(img)\n",
    "    img = enhancer.enhance(value)\n",
    "    return [img, f\"Brightness: value = {value}\"]\n",
    "\n",
    "def contrast(img, rng):\n",
    "    value = rng.uniform(0.3,1.7)\n",
    "    enhancer = ImageEnhance.Contrast(img)\n",
    "    img = enhancer.enhance(value)\n",
    "    return [img, f\"Constrast: value = {value}\"]\n",
    "\n",
    "def color(img, rng):\n",
    "    value = rng.uniform(0.3,1.7)\n",
    "    enhancer = ImageEnhance.Color(img)\n",
    "    img = enhancer.enhance(value)\n",
    "    return [img, f\"Color: value = {value}\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7082d3-454a-4af8-8884-0386a455c618",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7e0f88-cc6c-4b15-af81-166773b2be0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import nn,np\n",
    "\n",
    "\n",
    "class SimplifiedVGG(nn.Module):\n",
    "    def __init__(self, num_classes=10, dropout_rate=0.3):\n",
    "        super(SimplifiedVGG, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "    \n",
    "                nn.Conv2d(3, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "                \n",
    "                nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "                \n",
    "                nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "                \n",
    "                nn.Conv2d(256, 512, kernel_size=3, padding=1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(512, 512, kernel_size=3, padding=1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(512, 512, kernel_size=3, padding=1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "                \n",
    "                nn.Conv2d(512, 512, kernel_size=3, padding=1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(512, 512, kernel_size=3, padding=1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(512, 512, kernel_size=3, padding=1), nn.BatchNorm2d(512), nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            )\n",
    "\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 1 * 1, 2048), nn.BatchNorm1d(2048), nn.ReLU(inplace=True), nn.Dropout(0.2),\n",
    "            nn.Linear(2048, 1024), nn.BatchNorm1d(1024), nn.ReLU(inplace=True), nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 512), nn.BatchNorm1d(512), nn.ReLU(inplace=True), nn.Dropout(0.2),\n",
    "            nn.Linear(512, num_classes)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb1650c-2821-4c39-942d-2ac7e055e6cb",
   "metadata": {},
   "source": [
    "# basic configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657a0b6a-eab1-4ee0-958d-83e8abd624ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "K_FOLDS = 3\n",
    "PATIENCE = 7\n",
    "NUM_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a625e2-6019-4a38-8ada-d89f22cd7dbb",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebeb191-c3d6-4b23-8b02-efca3383e380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from train import Trainer\n",
    "\n",
    "\n",
    "def run_hyperparameter_tuning():\n",
    "   \n",
    "    os.makedirs(PARAMS_PATH, exist_ok=True)\n",
    "\n",
    "\n",
    "    param_space = {\n",
    "        \"Adam\": [\n",
    "            {\"learning_rate\": 0.001, \"batch_size\": 64},\n",
    "            {\"learning_rate\": 0.0005, \"batch_size\": 64},\n",
    "            {\"learning_rate\": 0.0005, \"batch_size\": 128}\n",
    "        ],\n",
    "        \"SGD\": [\n",
    "            {\"learning_rate\": 0.01, \"batch_size\": 32},\n",
    "            {\"learning_rate\": 0.005, \"batch_size\": 64},\n",
    "            {\"learning_rate\": 0.001, \"batch_size\": 128}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    for opt_type, configs in param_space.items():\n",
    "        best_acc = 0\n",
    "        best_config = None\n",
    "\n",
    "        for config in configs:\n",
    "            print(f\"\\nTrying Hyperparameters: {config} with Optimizer: {opt_type}\")\n",
    "            trainer = Trainer(\n",
    "                learning_rate=config[\"learning_rate\"],\n",
    "                batch_size=config[\"batch_size\"],\n",
    "                optimizer_type=opt_type\n",
    "            )\n",
    "            val_acc = trainer.train(return_best_val_acc=True)\n",
    "\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                best_config = config\n",
    "\n",
    "     \n",
    "        result = {\"best_acc\": best_acc, \"best_config\": best_config}\n",
    "        np.save(os.path.join(PARAMS_PATH, f\"best_params_{opt_type}.npy\"), result)\n",
    "\n",
    "        print(f\"\\n Best Hyperparameters for {opt_type}: {best_config} with Accuracy: {best_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3093e4-db6c-44f7-bc25-0bdca4add73d",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c9c64b-5f69-4a91-9e55-df73077312bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=0.2):\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d69437-5093-452c-bd02-9d32514de906",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\widetilde{y}_i &= \\lambda\\,\\mathbf{1}_{y_i} + (1 - \\lambda)\\,\\mathbf{1}_{y_j}, \\\\[6pt]\n",
    "\\widetilde{y}_i &:\\ \\text{mixed (soft) label for sample }i,\\\\\n",
    "\\mathbf{1}_{y_i} &:\\ \\text{one-hot vector of length }C\\text{ with a 1 at index }y_i,\\\\\n",
    "\\mathbf{1}_{y_j} &:\\ \\text{one-hot vector of length }C\\text{ with a 1 at index }y_j,\\\\\n",
    "y_i &:\\ \\text{ground-truth class label of sample }i,\\\\\n",
    "y_j &:\\ \\text{ground-truth class label of the sample paired to }i,\\\\\n",
    "\\lambda &:\\ \\text{mixing coefficient sampled from }\\mathrm{Beta}(\\alpha,\\alpha).\n",
    "\\end{align*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a38e3f-6112-4e23-b03f-b46aedc2e338",
   "metadata": {},
   "source": [
    "1. Learns better patterns\n",
    "2. Handles noise better\n",
    "3. Improves generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca7e2b2-49cd-4946-b38d-93f0b3e1fbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import torch, np, optim, DEVICE, NUM_CLASSES, NUM_EPOCHS, K_FOLDS, PATIENCE, KFold, DataLoader, Subset, load_dataset\n",
    "from model import SimplifiedVGG\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, learning_rate=0.001, batch_size=32, optimizer_type=\"Adam\"):\n",
    "        self.batch_size = batch_size\n",
    "        self.device = DEVICE\n",
    "        self.num_classes = NUM_CLASSES\n",
    "        self.num_epochs = NUM_EPOCHS\n",
    "        self.k_folds = K_FOLDS\n",
    "        self.patience = PATIENCE\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer_type = optimizer_type\n",
    "        self.train_loader, _ = load_dataset.load_dataset(batch_size=self.batch_size, path=\"/scratch/username\")\n",
    "        self.kf = KFold(n_splits=self.k_folds, shuffle=True, random_state=SEED)\n",
    "\n",
    "    def get_optimizer(self, model):\n",
    "        if self.optimizer_type == \"Adam\":\n",
    "            optimizer = optim.AdamW(model.parameters(), lr=self.learning_rate, weight_decay=1e-4)\n",
    "            scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "        elif self.optimizer_type == \"SGD\":\n",
    "            optimizer = optim.SGD(model.parameters(), lr=self.learning_rate, momentum=0.9, weight_decay=5e-4, nesterov=True)\n",
    "            scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "                optimizer,\n",
    "                max_lr=self.learning_rate,\n",
    "                steps_per_epoch=len(self.train_loader),\n",
    "                epochs=self.num_epochs\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported optimizer type\")\n",
    "        return optimizer, scheduler\n",
    "\n",
    "    def train(self, return_best_val_acc=False):\n",
    "        fold_results = []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(self.kf.split(range(len(self.train_loader.dataset)))):\n",
    "            print(f\"\\nFold {fold+1}/{self.k_folds}\")\n",
    "            \n",
    "            best_val_loss = np.inf\n",
    "            epochs_no_improve = 0\n",
    "            history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
    "\n",
    "            train_subset = Subset(self.train_loader.dataset, train_idx)\n",
    "            val_subset = Subset(self.train_loader.dataset, val_idx)\n",
    "            train_loader = DataLoader(train_subset, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
    "            val_loader = DataLoader(val_subset, batch_size=self.batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "            model = SimplifiedVGG().to(self.device)\n",
    "            criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.01)\n",
    "            optimizer, scheduler = self.get_optimizer(model)\n",
    "\n",
    "            for epoch in range(self.num_epochs):\n",
    "                model.train()\n",
    "                running_loss, correct_train, total_train = 0.0, 0, 0\n",
    "\n",
    "                for images, labels in train_loader:\n",
    "                    images, labels = images.to(self.device), labels.to(self.device).long()\n",
    "                    optimizer.zero_grad()\n",
    "                    images, labels_a, labels_b, lam = mixup_data(images, labels, alpha=0.2)\n",
    "                    outputs = model(images)\n",
    "                    loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    running_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    correct_train += (predicted == labels).sum().item()\n",
    "                    total_train += labels.size(0)\n",
    "\n",
    "               \n",
    "                model.eval()\n",
    "                correct_val, total_val, val_loss = 0, 0, 0.0\n",
    "                with torch.no_grad():\n",
    "                    for images, labels in val_loader:\n",
    "                        images, labels = images.to(self.device), labels.to(self.device).long()\n",
    "                        outputs = model(images)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        val_loss += loss.item()\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        correct_val += (predicted == labels).sum().item()\n",
    "                        total_val += labels.size(0)\n",
    "\n",
    "                val_accuracy = 100 * correct_val / total_val\n",
    "                print(f\"Epoch [{epoch+1}/{self.num_epochs}], Val Loss: {val_loss/len(val_loader):.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "                epoch_train_loss = running_loss / len(train_loader)\n",
    "                epoch_train_acc = 100 * correct_train / total_train\n",
    "                epoch_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "                if epoch_val_loss < best_val_loss:\n",
    "                    best_val_loss = epoch_val_loss\n",
    "                    epochs_no_improve = 0\n",
    "                    torch.save(model.state_dict(), f\"best_model_{self.optimizer_type}.pth\")\n",
    "                    print(f\"Val Loss Improved. Model saved at epoch {epoch+1}\")\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                    print(f\"No improvement for {epochs_no_improve} epoch(s)\")\n",
    "\n",
    "                if epochs_no_improve >= self.patience:\n",
    "                    print(f\"\\nEarly stopping at epoch {epoch+1} due to no improvement for {self.patience} epochs.\")\n",
    "                    break\n",
    "\n",
    "                history[\"train_loss\"].append(epoch_train_loss)\n",
    "                history[\"val_loss\"].append(epoch_val_loss)\n",
    "                history[\"train_acc\"].append(epoch_train_acc)\n",
    "                history[\"val_acc\"].append(val_accuracy)\n",
    "\n",
    "                if scheduler:\n",
    "                    scheduler.step()\n",
    "\n",
    "            fold_results.append(val_accuracy)\n",
    "            np.save(f\"training_history_{self.optimizer_type}_fold_{fold}.npy\", history)\n",
    "            print(f\"Training history saved for fold {fold}.\")\n",
    "\n",
    "        avg_val_acc = np.mean(fold_results)\n",
    "        print(f\"\\n Average Validation Accuracy ({self.optimizer_type}): {avg_val_acc:.2f}%\")\n",
    "        print(f\"Estimated Test Error: {100 - avg_val_acc:.2f}%\")\n",
    "\n",
    "        if return_best_val_acc:\n",
    "            return max(fold_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc26c63",
   "metadata": {},
   "source": [
    "# evaluaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacd465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import torch, sns, plt, os, np, confusion_matrix, DEVICE, NUM_CLASSES, load_dataset, PLOT_PATH, EVAL_PATH\n",
    "from model import SimplifiedVGG\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_curve, auc, precision_recall_curve, f1_score\n",
    "from collections import Counter\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "def evaluate_model(model_name, visualize=True):\n",
    "    model = SimplifiedVGG().to(DEVICE)\n",
    "    model.load_state_dict(torch.load(f\"best_model_{model_name}.pth\"))\n",
    "    model.eval()\n",
    "\n",
    "    _, test_loader = load_dataset.load_dataset(batch_size=32, path=\"/scratch/username\")\n",
    "\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE).long()\n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = outputs.max(1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"\\nEvaluation for Optimizer: {model_name}\")\n",
    "    print(f\"Test Accuracy: {acc * 100:.2f}%\")\n",
    "    report = classification_report(all_labels, all_preds, digits=4)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    os.makedirs(EVAL_PATH, exist_ok=True)\n",
    "    with open(os.path.join(EVAL_PATH, f\"classification_report_model_{model_name}.txt\"), \"w\") as f:\n",
    "        f.write(f\"Test Accuracy: {acc * 100:.2f}%\\n\")\n",
    "        f.write(report)\n",
    "\n",
    "    if visualize:\n",
    "        visualize_results(all_labels, all_preds, all_probs, test_loader, model_name, acc)\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "def visualize_results(all_labels, all_preds, all_probs, test_loader, model_name, acc):\n",
    "    os.makedirs(PLOT_PATH, exist_ok=True)\n",
    "    classes = list(range(NUM_CLASSES))\n",
    "\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(PLOT_PATH, f\"confusion_matrix_{model_name}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = {}\n",
    "\n",
    "    for opt in [\"Adam\", \"SGD\"]:\n",
    "        acc = evaluate_model(model_name=opt, visualize=True)\n",
    "        results[opt] = acc * 100\n",
    "\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    for opt, acc in results.items():\n",
    "        print(f\"{opt}: {acc:.2f}% Test Accuracy\")\n",
    "\n",
    "    best = max(results, key=results.get)\n",
    "    print(f\"\\nBest Optimizer: {best} with Accuracy: {results[best]:.2f}%\")\n",
    " \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(x=list(results.keys()), y=list(results.values()), palette=\"viridis\")\n",
    "    plt.title(\"Test Accuracy Comparison\")\n",
    "    for i, v in enumerate(results.values()):\n",
    "        plt.text(i, v + 1, f\"{v:.2f}%\", ha=\"center\", fontsize=12)\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(PLOT_PATH, \"evaluation_results_chart.png\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    PLOT_PATH = \"plots\"\n",
    "    os.makedirs(PLOT_PATH, exist_ok=True)\n",
    "\n",
    "    optimizers = [\"Adam\", \"SGD\"]\n",
    "    folds = [0, 1, 2]\n",
    "\n",
    "    for opt in optimizers:\n",
    "        all_acc = []\n",
    "        all_loss = []\n",
    "\n",
    "        for fold in folds:\n",
    "            file = f\"training_history_{opt}_fold_{fold}.npy\"\n",
    "            if not os.path.exists(file):\n",
    "                print(f\"Missing file: {file}\")\n",
    "                continue\n",
    "\n",
    "            history = np.load(file, allow_pickle=True).item()\n",
    "            acc = history[\"train_acc\"] \n",
    "            loss = history[\"train_loss\"]\n",
    "\n",
    "            all_acc.append(acc)\n",
    "            all_loss.append(loss)\n",
    "\n",
    "            # Training Accuracy - per fold\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.lineplot(x=range(len(acc)), y=acc)\n",
    "            plt.title(f\"Training Accuracy - {opt} - Fold{fold}\")\n",
    "            plt.xlabel(\"Epochs\")\n",
    "            plt.ylabel(\"Accuracy (%)\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(PLOT_PATH, f\"training_accuracy_{opt}_fold_{fold}.png\"))\n",
    "            plt.close()\n",
    "\n",
    "            # Training Loss - per fold\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.lineplot(x=range(len(loss)), y=loss)\n",
    "            plt.title(f\"Training Loss - {opt} - Fold{fold}\")\n",
    "            plt.xlabel(\"Epochs\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(PLOT_PATH, f\"training_loss_{opt}_fold_{fold}.png\"))\n",
    "            plt.close()\n",
    "\n",
    "        if all_acc and all_loss:\n",
    "            # Truncate to minimum length across folds\n",
    "            min_len = min(len(a) for a in all_acc)\n",
    "            all_acc = [a[:min_len] for a in all_acc]\n",
    "            all_loss = [l[:min_len] for l in all_loss]\n",
    "\n",
    "            avg_acc = np.mean(all_acc, axis=0)\n",
    "            avg_loss = np.mean(all_loss, axis=0)\n",
    "\n",
    "            # Training Accuracy - averaged\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.lineplot(x=range(len(avg_acc)), y=avg_acc)\n",
    "            plt.title(f\"Training Accuracy Average - {opt} - Fold{fold}\")\n",
    "            plt.xlabel(\"Epochs\")\n",
    "            plt.ylabel(\"Accuracy (%)\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(PLOT_PATH, f\"training_accuracy_{opt}_avg.png\"))\n",
    "            plt.close()\n",
    "\n",
    "            # Training Loss - averaged\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.lineplot(x=range(len(avg_loss)), y=avg_loss)\n",
    "            plt.title(f\"Training Loss Average - {opt} - Fold{fold}\")\n",
    "            plt.xlabel(\"Epochs\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(PLOT_PATH, f\"training_loss_{opt}_avg.png\"))\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb30c09-24c3-4763-9c2d-a9191f20e5e0",
   "metadata": {},
   "source": [
    "# Predicting Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc8a95f-3059-4ebb-b59b-ba49bb17531c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image,ImageDraw, ImageFont\n",
    "import os\n",
    "from model import SimplifiedVGG\n",
    "from config import DEVICE, NUM_CLASSES\n",
    "\n",
    "IMAGE_SIZE = 56\n",
    "MODEL_PATH = \"best_model_Adam.pth\"\n",
    "CLASS_NAMES = [\"bike\",\"bottle\",\"chair\",\"fork\",\"knife\",\"mug/cup\",\"plant\",\"shoe\",\"spoon\",\"T-shirt\"]\n",
    "\n",
    "tensor_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "    # transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])\n",
    "\n",
    "def predict_image():\n",
    "    raw_img = []\n",
    "    processed_img = []\n",
    "    img_paths = os.listdir(\"images\")\n",
    "    n = len(img_paths)\n",
    "    input_tensor = torch.zeros(n,3,IMAGE_SIZE,IMAGE_SIZE)\n",
    "    for i,img_path in enumerate(img_paths):\n",
    "        raw_img += [Image.open(\"images/\"+img_path).convert(\"RGB\")]\n",
    "        processed_img += [transforms.functional.center_crop(raw_img[i],min(raw_img[i].size)).resize((IMAGE_SIZE,IMAGE_SIZE))]    \n",
    "        input_tensor[i] = tensor_transform(processed_img[i])\n",
    "\n",
    "    model = SimplifiedVGG(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        predicted_class = torch.argmax(probs, dim=1)\n",
    "        confidence = torch.max(probs,dim=1)\n",
    "\n",
    "    output_img = Image.new(\"RGB\",(250,250*n),color=\"white\")\n",
    "    draw = ImageDraw.Draw(output_img)\n",
    "    font = ImageFont.truetype(\"ARIAL.ttf\",15)\n",
    "    for i in range(n):\n",
    "        text = \"\\n\".join(f\"{CLASS_NAMES[j]}: {prob:.2%}\" for prob,j in zip(*torch.sort(probs[i],descending=True)))\n",
    "        draw.text((100,i*250),text,(0,0,0),font)\n",
    "        output_img.paste(processed_img[i],(0,i*250))\n",
    "        \n",
    "    output_img.save(\"prediction.jpeg\")\n",
    "    print(\"Prediction saved as prediction.jpeg\")\n",
    "\n",
    "predict_image()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
